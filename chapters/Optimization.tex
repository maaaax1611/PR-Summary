\section{Optimization}
\subsection{Convexity}
A function \( f: \mathbb{R}^n \rightarrow \mathbb{R} \) is called \textbf{convex} if for all \( x, y \in \mathbb{R}^n \) and for all \( \theta \in [0, 1] \), the following condition holds:

\begin{summarybox}{Definition: Convexity \& Concavity}
    A function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{convex} if the domain $\operatorname{dom}(f)$ of $f$ is a convex set and if $\forall \mathbf{x}, \mathbf{y} \in \operatorname{dom}(f)$, and $\theta$ with $0 \leq \theta \leq 1$, we have:
    \begin{equation}
        f(\theta\mathbf{x} + (1-\theta)\mathbf{y}) \leq \theta f(\mathbf{x}) + (1-\theta)f(\mathbf{y})
    \end{equation}
    
    A function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{concave} if $-f$ is convex.
\end{summarybox}

This means that the line segment connecting any two points on the graph of the function lies \textbf{above or on the graph itself}. Convex functions have the property that any local minimum is also a global minimum, which is particularly useful in optimization problems.

\subsection{Unconstrained Optimization}
In unconstrained optimization, we aim to find the minimum (or maximum) of a function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ without any restrictions on the variable values.
Typically we assume that the function \( f \) is twice differentiable and convex.
The unconstrained otpimization is the solution to the minimization problem
\begin{equation}
    \vec{x}^* = \arg \min_{\vec{x}} f(\vec{x})
\end{equation}
Where $\vec{x}$ denotes the optimal point.
For this family of functions, a necessary and sufficient condition for a minimum are the zero-crossings of the gradient:
\begin{equation}
    \nabla f(\vec{x}^*) = 0
\end{equation}
Most of the time, we cannot find a closed-form solution to this equation.
So we need to follow an iterative approach:
\begin{align*}
    \text{initialization} & \qquad \mathbf{x}^{(0)} \\
    \text{iteration step} & \qquad \mathbf{x}^{(k+1)} = g(\mathbf{x}^{(k)})
\end{align*}

where $g : \mathbb{R}^d \rightarrow \mathbb{R}^d$ is the update function.

The iterations terminate, if
\[
\|\mathbf{x}^{(k+1)} - \mathbf{x}^{(k)}\| < \varepsilon,
\]
for some small threshold \(\varepsilon > 0\). This means that there is not furher significant change in the variable values, indicating convergence to a solution.
Some common iterative optimization methods will be discussed below.

\subsubsection{Descent Methods}
The basic idea of descent methods is to iteratively take small steps $\Delta \vec{x}^{(k)}$ in the direction of \textbf{steepest descent} (i.e., the negative gradient) to minimize the function value:
\begin{equation}
    \vec{x}^{(k+1)} = \vec{x}^{(k)} + t^{(k)} \Delta \vec{x}^{(k)}
\end{equation}
where
\begin{align}
    \Delta\vec{x}^{(k)} &\in \mathbb{R}^d : && \text{is the search direction in the k-th iteration} \\
    t^{(k)} &\in \mathbb{R} : && \text{denotes the step length in the k-th iteration}
\end{align}
and where we expect:
\begin{equation}
    f(\vec{x}^{(k+1)}) < f(\vec{x}^{(k)}) \quad \text{except} \quad \vec{x}^{(k+1)} = \vec{x}^{(k)} = \vec{x}^*
\end{equation}
\textbf{Finding a suitable step size}\\
It is important to choose an appropriate step size \( t^{(k)} \) to ensure convergence and avoid overshooting the minimum.
If $t$ is too large, we might overshoot the minimum, while if it is too small, convergence can be very slow or get stuck in local minima.
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=\textwidth]{images/learning_rate.png}
    \caption{Learning Rate Illustration}
    \label{fig:learning_rate}
\end{figure}
The procedure to find a suitable step size is called \textbf{line search}.
You can basically see this as a function $F(\vec{x}^{(k)}) + t^{(k)} \Delta \vec{x}^{(k)}$ where $\Delta \vec{x}^{(k)}$ and $\vec{x}^{(k)}$ are fixed and we only vary $t^{(k)}$ to find the minimum of this 1D function.
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.6\textwidth]{images/BacktrackingLineSearch.png}
    \caption{Backtracking line search}
    \label{fig:backtracking_line_search}
\end{figure}
A common method for line search is \textbf{backtracking line search (Armijo-Goldstein Algorithm)}, which starts with an initial step size and iteratively reduces it until a sufficient decrease in the function value is observed.

\subsubsection{Gradient Descent}
A natutral choice for the search direction is the negative gradient of the function at the current point:
\begin{equation}
    x^{(k+1)} = x^{(k)} - t^{(k)} \nabla f(x^{(k)})
\end{equation}
You can use either a fixed step size or perform a line search to determine the optimal step size at each iteration.\\

\textbf{Algorithm:}\\
\textbf{Input:} function $f$, initial estimate $\mathbf{x}^{(0)}$ \\
\textbf{initialize:} $k := 0$ \\
\textbf{repeat}
\begin{enumerate}
    \item Set descent direction: $\Delta\mathbf{x}^{(k)} = - \nabla f(\mathbf{x}^{(k)})$
    \item Line search (1-D optimization):
    \[
    t^{(k)} = \arg\min_{t \ge 0} f(\mathbf{x}^{(k)} + t \cdot \Delta\mathbf{x}^{(k)})
    \]
    \item Update:
    \[
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + t^{(k)} \Delta\mathbf{x}^{(k)}
    \]
    \item $k := k + 1$
\end{enumerate}
\textbf{until} $\|\mathbf{x}^{(k)} - \mathbf{x}^{(k-1)}\|_2 < \varepsilon$ \\
\textbf{Output:} $\mathbf{x}^{(k)}$


\subsubsection{Normalized Steepest Descent}
In normalilzed steepest descent, the search direction is given by the following optimization problem:
\begin{equation}
    \Delta \vec{x} = \arg\min_{u} \nabla f(\vec{x})^T \vec{u} \quad \text{s.t.} \quad \|\vec{u}\| = 1
\end{equation}
This means that we compute the inner product of the gradient and a unit vector $\vec{u}$ (project the gradient onto $\vec{u}$) and choose the direction with the steepest descent.\\

\textbf{Algorithm:}\\
\textbf{Input:} function $f$, initial estimate $\mathbf{x}^{(0)}$, norm $\|\cdot\|$ \\
\textbf{initialize:} $k := 0$ \\
\textbf{repeat}
\begin{enumerate}
    \item Compute highest descent direction:
    \[
    \Delta\mathbf{x}^{(k)} = \arg\min_{\mathbf{u}} \{ \nabla f(\mathbf{x}^{(k)})^T \mathbf{u} ; \|\mathbf{u}\| = 1 \}
    \]
    \item Line search (1-D optimization):
    \[
    t^{(k)} = \arg\min_{t \ge 0} f(\mathbf{x}^{(k)} + t \cdot \Delta\mathbf{x}^{(k)})
    \]
    \item Update:
    \[
    \mathbf{x}^{(k+1)} = \mathbf{x}^{(k)} + t^{(k)} \Delta\mathbf{x}^{(k)}
    \]
    \item $k := k + 1$
\end{enumerate}
\textbf{until} $\|\mathbf{x}^{(k)} - \mathbf{x}^{(k-1)}\| < \varepsilon$ \\
\textbf{Output:} $\mathbf{x}^{(k)}$
If you choose the L2-norm, the unit ball is a circle (2D) or a sphere (3D).
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.5\textwidth]{images/UnitBallL2.png}
    \caption{Unit ball in L2 norm}
    \label{fig:unit_ball_l2}
\end{figure}
In this case, the steepest descent direction is simply the negative gradient direction. 
Therefore we get the same update rule as in gradient descent.\\

If you choose the L1-norm, the unit ball is a diamond (2D) or an octahedron (3D).
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=\textwidth]{images/UnitBallL1.png}
    \caption{Unit ball in L1 norm}
    \label{fig:unit_ball_l1}
\end{figure}
In this case, the steepest descent direction corresponds to the coordinate axis clostest to the negative gradient direction.
This leads to a coordinate descent method, where we optimize along one coordinate axis in each iteration:
\begin{align*}
    \Delta\mathbf{x} &= \arg\min_{\mathbf{u}} \left\{ \nabla f(\mathbf{x})^T \mathbf{u} \, ; \, \|\mathbf{u}\|_1 = 1 \right\} \\
    &= -\operatorname{sgn} \left( \frac{\partial}{\partial x_i} f(\mathbf{x}) \right) \mathbf{e}_i
\end{align*}
where $e_i$ is the unit vector along the coordinate axis with the highest absolute gradient value.\\

If you choose the $\text{L}_\infty$ norm, the unit ball is a square (2D) or a cube (3D).
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.8\textwidth]{images/UnitBallLInf.png}
    \caption{Unit ball in L-Infinity norm}
    \label{fig:unit_ball_linf}
\end{figure}
In this case, the direction of steepest descent is always aligned withe the diagonal of the unit cube (\ang{0}).\\

If you choose the $\text{L}_\text{P}$ norm, the unit ball is a rounded square (2D) or a rounded cube (3D).
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.5\textwidth]{images/UnitBallLP.png}
    \caption{Unit ball in L-P norm}
    \label{fig:unit_ball_lp}
\end{figure}
Using a quadratic norm defined by a positive definite matrix $\mathbf{P}$ ($\|\vec{u}\|_{\mathbf{P}} = \sqrt{\vec{u}^T \mathbf{P} \vec{u}}$), the steepest descent direction is:

\begin{equation}
    \Delta\vec{x} = -\mathbf{P}^{-1} \nabla f(\vec{x})
\end{equation}

\textbf{Connection to LDA:}
Just like in LDA, where we transform the feature space to make the class distributions spherical (whitening) using the covariance matrix, this optimization step uses $\mathbf{P}^{-1}$ to account for the geometry of the function.
\begin{itemize}
    \item If $\mathbf{P} = \mathbf{I}$: Standard Gradient Descent (assumes spherical geometry).
    \item If $\mathbf{P} = \nabla^2 f(\vec{x})$ (Hessian): Newton's Method (accounts for local curvature).
\end{itemize}

\subsubsection{Newton's Method}
Newton's method uses second-order information (the Hessian matrix) to find the search direction.
It uses the second-order Taylor polynomial to approximate the function $f(x)$ at the current point $x^{(k)}$ and finds the minimum of this approximation.
The minimum is the new point $x^{(k+1)}$.
\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.5\textwidth]{images/Taylor.png}
    \caption{Newtons Method using 2nd order Taylor approximation}
    \label{fig:newtons_method_taylor}
\end{figure}
The second-order Taylor approximation of $f(\vec{x})$ is given by:
\begin{equation}
    f(\vec{x} + \Delta \vec{x}) \approx f(\vec{x}) + \nabla f(\vec{x})^T \Delta \vec{x} + \frac{1}{2} \Delta \vec{x}^T ()\nabla^2 f(\vec{x})) \Delta \vec{x} 
\end{equation}
To find the minimum of this approximation, we set the gradient with respect to $\Delta \vec{x}$ to zero (works because the function is convex) and resolve for $\Delta \vec{x}$:
\begin{equation}
    \Delta \vec{x} = - \underbrace{(\nabla^2 f(\vec{x}))^{-1}}_{\text{inverse of the Hessian}} \nabla f(\vec{x})
\end{equation}
\textbf{Note:} Have a look at the steepest descent method using the \textbf{$\text{L}_\text{P}$}-norm. 
When $P$ is the Hessian matrix ($\mathbf{P} = \nabla^2 f(\vec{x})$), we get the same search direction as in Newton's method.

\newpage
\subsection{Constrained Optimization}
We consider the primal optimization problem:
\begin{align*}
    \text{minimize} \quad & f_0(\mathbf{x}) \\
    \text{subject to} \quad & f_i(\mathbf{x}) \leq 0, \quad i = 1, 2, \dots, m \\
    & h_i(\mathbf{x}) = 0, \quad i = 1, 2, \dots, p
\end{align*}
\textbf{Note:} $f_0(\vec{x})$ is not required to be convex!

To optimize a function with constraints we have to use Lagrange-Multipliers.
The Lagrangian $L$ is defined as:
\begin{equation}
    L(\vec{x}, \vec{\lambda}, \vec{\nu}) = f_0(\vec{x}) + \underbrace{\sum_{i=1}^m \lambda_i f_i(\vec{x})}_{\text{sum over all inequality constraints}} + \underbrace{\sum_{i=1}^p \nu_i h_i(\vec{x})}_{\text{sum over all equality constraints}}
\end{equation}
We add the constraints and weight them with multipliers $\lambda, \nu$ (Langrange multipliers or simply dual variables).
The optimization problem can then be reformulated using the \textbf{Lagrange dual function}:
\begin{align*}
    g(\vec{\lambda}, \vec{\nu}) &= \inf_{\vec{x}} L(\vec{x}, \vec{\lambda}, \vec{\nu}) \\
    &= \inf_{\vec{x}} \left( f_0(\vec{x}) + \sum_{i=1}^m \lambda_i f_i(\vec{x}) + \sum_{i=1}^p \nu_i h_i(\vec{x}) \right)
\end{align*}
\begin{summarybox}{Definition: Infimum ($\inf$)}
    The \textbf{infimum} is the greatest lower bound of a set. Unlike the minimum, it exists even for open sets (e.g., for the set $\{x \in \mathbb{R} \mid x > 0\}$, the minimum does not exist, but the infimum is 0).
\end{summarybox}
\textbf{Important Properties:}
\begin{itemize}
    \item The Lagrange dual function is always \textbf{concave}, even if the original primal problem is not convex!
    \item \textbf{Pointwise Affine:} The dual function is a pointwise affine function in the dual variables.
    \begin{itemize}
        \item \textit{Explanation:} If we fix $\vec{x}$, the Lagrangian $L(\vec{x}, \vec{\lambda}, \vec{\nu})$ becomes just a linear equation in terms of $\vec{\lambda}$ and $\vec{\nu}$ (like $a + b\lambda + c\nu$). It describes a flat plane.
    \end{itemize}
    \item It provides a \textbf{lower bound} on the optimal value $p^*$ of the primal problem. For any $\vec{\lambda} \succeq 0$ and any $\vec{\nu}$, the following holds:
    \begin{equation}
        g(\vec{\lambda}, \vec{\nu}) \leq p^*
    \end{equation}
\end{itemize}
This leads us to the \textbf{Lagrange Dual Problem}, where we try to find the best (highest) lower bound:
\begin{align*}
    \text{maximize} \quad & g(\vec{\lambda}, \vec{\nu}) \\
    \text{subject to} \quad & \vec{\lambda} \succeq 0
\end{align*}

\begin{summarybox}{Optimal Duality Gap}
    Let $p^*$ be the optimal value of the primal problem and $d^*$ the optimal value of the Lagrange dual problem.

    \begin{itemize}
        \item The difference $p^* - d^*$ is the \textbf{optimal duality gap}.
        \item If $p^* = d^*$, the duality gap is zero. In this case we talk about \textbf{strong duality}.
        \item If $p^* > d^*$, we have \textbf{weak duality}.
    \end{itemize}
\end{summarybox}

The duality gap is always zero when \textbf{Slater's Condition} is fulfilled:
\begin{summarybox}{Theorem: Slater's Condition}
    For a \textbf{convex} optimization problem, strong duality ($p^* = d^*$) holds if there exists a strictly feasible point $\mathbf{x}$ such that:
    \begin{equation}
        f_i(\mathbf{x}) < 0 \quad \forall i=1,\dots,m \quad \text{and} \quad A\mathbf{x} = \mathbf{b}
    \end{equation}
\end{summarybox}
\textbf{Intuition:}
To guarantee strong duality, we need at least one "strictly feasible" point.
\begin{itemize}
    \item It must lie \textbf{strictly inside} the inequality constraints ($f_i(\mathbf{x}) < 0$), meaning it does not touch these boundaries.
    \item At the same time, it must \textbf{exactly satisfy} the linear equality constraints ($A\mathbf{x} = \mathbf{b}$).
\end{itemize}
In simple terms: You need a point that walks exactly on the required path ($A\mathbf{x}=\mathbf{b}$) without touching the walls of the feasible region ($f_i(\mathbf{x}) < 0$).
\\

If $\vec{x}^*$ is a primal optimal point and $\vec{\lambda}^*, \vec{\nu}^*$ are dual optimal points with a zero duality gap, the gradient of the Lagrangian $L(\vec{x}, \vec{\lambda}^*, \vec{\nu}^*)$ with respect to $\vec{x}$ must be zero at $\vec{x}^*$:
\begin{equation}
    \nabla_x L(\vec{x}^*, \vec{\lambda}^*, \vec{\nu}^*) = \nabla f_0(\vec{x}^*) + \sum_{i=1}^m \lambda_i^* \nabla f_i(\vec{x}^*) + \sum_{i=1}^p \nu_i^* \nabla h_i(\vec{x}^*) = 0
    \label{lagrangian_zero_grad}
\end{equation}
\\
\textbf{KKT Conditions:}\\
If strong duality holds (e.g., because Slater's condition is met), any pair of optimal primal ($\vec{x}^*$) and dual ($\vec{\lambda}^*, \vec{\nu}^*$) variables \textbf{must} satisfy the following four conditions:

\begin{enumerate}
    \item Primal constrains:
    \begin{align*}
        f_i(\vec{x}) &\leq 0, \quad i = 1, 2, ..., m \\
        h_i(\vec{x}) &= 0, \quad i = 1, 2, ..., p
    \end{align*}

    \item Dual constraints: $\vec{\lambda} \geq 0$

    \item Complementary slackness: $\lambda_i \cdot f_i(\vec{x}) = 0$ (Either the constraint is active, or the corresponding dual variable is zero).

    \item Gradient of Lagrangian is zero (as shown in \ref{lagrangian_zero_grad})
\end{enumerate}


To summarize all this in simple terms:
\begin{itemize}
    \item For any optimization problem with differentiable objective and constraint functions for
which strong duality obtains, any pair of primal and dual optimal points must satisfy
the KKT conditions.
    \item Consequently, if we can find points that satisfy the KKT conditions, we have found optimal primal and dual points with zero duality gap.
    \item The KKT conditions are necessary for optimality in convex problems with strong duality, and they are also sufficient.
\end{itemize}

\newpage