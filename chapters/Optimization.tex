% Datei: chapters/03_optimization.tex

\section{Optimization}

Optimization is crucial for many solutions in pattern recognition (e.g., training classifiers). We distinguish between unconstrained and constrained optimization problems.

\subsection{Convexity}
A function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is called \textbf{convex} if for all $\vec{x}, \vec{y} \in \mathbb{R}^n$ and for all $\theta \in [0, 1]$, the following condition holds:

\begin{summarybox}{Definition: Convexity \& Concavity}
    A function $f : \mathbb{R}^d \rightarrow \mathbb{R}$ is \textbf{convex} if the domain $\operatorname{dom}(f)$ is a convex set and if $\forall \vec{x}, \vec{y} \in \operatorname{dom}(f)$, and $\theta$ with $0 \leq \theta \leq 1$, we have:
    \begin{equation}
        f(\theta\vec{x} + (1-\theta)\vec{y}) \leq \theta f(\vec{x}) + (1-\theta)f(\vec{y})
    \end{equation}
    A function is \textbf{concave} if $-f$ is convex.
\end{summarybox}

\textbf{Implication:} For convex functions, any local minimum is also a global minimum. This property is particularly useful because gradient-based methods won't get stuck in suboptimal local minima.

\subsection{Unconstrained Optimization}
Here, we aim to find the minimum of a function $f(\vec{x})$ without any restrictions on $\vec{x}$. Typically, we assume $f$ is twice differentiable and convex.

\begin{equation}
    \vec{x}^* = \operatorname{argmin}_{\vec{x}} f(\vec{x})
\end{equation}

A necessary and sufficient condition for the minimum is the zero-crossing of the gradient:
\begin{equation}
    \nabla f(\vec{x}^*) = 0
\end{equation}
Since a closed-form solution is often impossible, we use iterative approaches:
\begin{align*}
    \text{initialization:} & \quad \vec{x}^{(0)} \\
    \text{iteration step:} & \quad \vec{x}^{(k+1)} = \vec{x}^{(k)} + t^{(k)} \Delta \vec{x}^{(k)}
\end{align*}
where $\Delta \vec{x}^{(k)}$ is the \textbf{search direction} and $t^{(k)}$ is the \textbf{step size}.

\subsubsection{Finding a Suitable Step Size (Line Search)}
Choosing the correct step size $t^{(k)}$ is crucial:
\begin{itemize}
    \item \textbf{Too small:} Convergence is extremely slow.
    \item \textbf{Too large:} The algorithm might overshoot the minimum or diverge.
\end{itemize}
Instead of finding the exact optimal $t$ (which is computationally expensive), we use \textbf{inexact line search} methods like \textbf{Backtracking Line Search} (Armijo-Goldstein).

\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.6\textwidth]{images/BacktrackingLineSearch.png}
    \caption{Backtracking line search}
    \label{fig:backtracking_line_search}
\end{figure}
The goal is to find a step size $t$ that, that puts us below the red line, ensuring that the function value decreases sufficiently (not just barely) relative to the step size.
\begin{summarybox}{Armijo-Goldstein Condition (Backtracking)}
    We start with a large step size ($t=1$) and iteratively reduce it ($t := \beta t$) until the function value decreases sufficiently. The condition is:
    \begin{equation}
        f(\vec{x} + t\Delta\vec{x}) \le f(\vec{x}) + \alpha t \nabla f(\vec{x})^T \Delta\vec{x}
    \end{equation}
    Where $\alpha \in (0, 0.5)$ defines the required "steepness" of the descent.
\end{summarybox}

\subsubsection{Gradient Descent}
\label{sec:gradient_descent}
The most natural choice for the search direction is the direction of steepest descent, which is the negative gradient.
\begin{equation}
    \Delta \vec{x}^{(k)} = - \nabla f(\vec{x}^{(k)})
\end{equation}

\textbf{Algorithm: Gradient Descent}
\begin{enumerate}
    \item Set direction: $\Delta \vec{x}^{(k)} = - \nabla f(\vec{x}^{(k)})$
    \item Line search (find optimal step size $t$):
    \[ t^{(k)} = \operatorname{argmin}_{t \ge 0} f(\vec{x}^{(k)} + t \Delta \vec{x}^{(k)}) \]
    (Usually approximated via \textbf{Backtracking Line Search / Armijo-Goldstein}).
    \item Update: $\vec{x}^{(k+1)} = \vec{x}^{(k)} + t^{(k)} \Delta \vec{x}^{(k)}$
    \item Repeat until convergence ($\| \vec{x}^{(k)} - \vec{x}^{(k-1)} \| < \epsilon$).
\end{enumerate}

\subsubsection{Normalized Steepest Descent (General Norms)}
Ideally, we want the direction that gives the largest decrease in the linear approximation of $f$. This depends on the chosen norm $\|\cdot\|$.
\begin{equation}
    \Delta \vec{x} = \operatorname{argmin}_{\vec{u}} \{ \nabla f(\vec{x})^T \vec{u} \mid \|\vec{u}\| = 1 \}
\end{equation}

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/UnitBallL2.png}
        \caption{Unit ball in $L_2$ norm}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/UnitBallL1.png}
        \caption{Unit ball in $L_1$ norm}
    \end{minipage}
\end{figure}

\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/UnitBallLP.png}
        \caption{Unit ball in $L_P$ norm}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{images/UnitBallLInf.png}
        \caption{Unit ball in $L_\infty$ norm}
    \end{minipage}
\end{figure}

\begin{itemize}
    \item \textbf{$L_2$-Norm:} Direction is $-\nabla f(\vec{x})$ (Standard Gradient Descent). Unit ball is a sphere.
    \item \textbf{$L_1$-Norm:} Direction is along the coordinate axis with the largest absolute gradient value (Coordinate Descent). Unit ball is a diamond.
    \item \textbf{$L_\infty$-Norm:} Direction points towards the corners of the unit hypercube (e.g., vector with entries $\pm 1$). Unit ball is a square/cube.
    \item \textbf{$L_P$-Norm (Quadratic Norm):} Defined by a positive definite matrix $\mathbf{P}$ as $\|\vec{u}\|_{\mathbf{P}} = \sqrt{\vec{u}^T \mathbf{P} \vec{u}}$.
    \begin{equation}
        \Delta \vec{x} = - \mathbf{P}^{-1} \nabla f(\vec{x})
    \end{equation}
    This transforms the space to make the contours spherical before taking the gradient step (similar to whitening in LDA). The update direction aligns with the largest principal component in the transformed space.

    \textbf{Crucial Connection:} If we set $\mathbf{P} = \nabla^2 f(\vec{x})$ (the Hessian), we effectively perform steepest descent in the local curvature norm, which yields \textbf{Newton's Method}.
\end{itemize}



\subsubsection{Newton's Method}
\label{sec:Newtons_method}
Newton's method finds the search direction by approximating the function $f(\vec{x})$ locally using a \textbf{second-order Taylor polynomial}:
\begin{equation}
    f(\vec{x} + \Delta\vec{x}) \approx f(\vec{x}) + \nabla f(\vec{x})^T \Delta\vec{x} + \frac{1}{2} \Delta\vec{x}^T \nabla^2 f(\vec{x}) \Delta\vec{x}
\end{equation}

\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=0.5\textwidth]{images/Taylor.png}
    \caption{Newtons Method using 2nd order Taylor approximation}
    \label{fig:newtons_method_taylor}
\end{figure}

We find the optimal step $\Delta\vec{x}$ by setting the derivative of this approximation to zero:
\begin{align*}
    \nabla_{\Delta\vec{x}} (\dots) &= \nabla f(\vec{x}) + \nabla^2 f(\vec{x}) \Delta\vec{x} = 0 \\
    \Rightarrow \quad \Delta\vec{x} &= - (\nabla^2 f(\vec{x}))^{-1} \nabla f(\vec{x})
\end{align*}

\textbf{Key Difference:} While Gradient Descent assumes the local geometry is a sphere (linear approximation), Newton's Method accounts for the local \textbf{curvature} (ellipsoidal geometry) given by the Hessian $\nabla^2 f(\vec{x})$.

\subsection{Constrained Optimization}
We consider the primal optimization problem with constraints:
\begin{align*}
    \text{minimize} \quad & f_0(\vec{x}) \\
    \text{subject to} \quad & f_i(\vec{x}) \leq 0, \quad i = 1, \dots, m \\
    & h_i(\vec{x}) = 0, \quad i = 1, \dots, p
\end{align*}
\textit{Note: $f_0$ is not required to be convex yet.}

\subsubsection{The Lagrangian \& Dual Function}
To handle constraints, we define the \textbf{Lagrangian} $L$:
\begin{equation}
    L(\vec{x}, \vec{\lambda}, \vec{\nu}) = f_0(\vec{x}) + \sum_{i=1}^m \lambda_i f_i(\vec{x}) + \sum_{i=1}^p \nu_i h_i(\vec{x})
\end{equation}
We formulate the \textbf{Lagrange dual function} $g(\vec{\lambda}, \vec{\nu})$ by minimizing $L$ over $\vec{x}$:
\begin{equation}
    g(\vec{\lambda}, \vec{\nu}) = \inf_{\vec{x}} L(\vec{x}, \vec{\lambda}, \vec{\nu})
\end{equation}

\begin{summarybox}{Properties of the Dual Function}
    \begin{itemize}
        \item $g(\vec{\lambda}, \vec{\nu})$ is \textbf{always concave}, even if the primal problem is not convex.
        \item It provides a \textbf{lower bound} on the optimal primal value $p^*$: $g(\vec{\lambda}, \vec{\nu}) \leq p^*$.
    \end{itemize}
\end{summarybox}

This leads to the \textbf{Dual Problem}: Find the best lower bound by maximizing $g$.
\[ \text{maximize } g(\vec{\lambda}, \vec{\nu}) \quad \text{subject to } \vec{\lambda} \succeq 0 \]

\subsubsection{Strong Duality \& Slater's Condition}
\begin{itemize}
    \item \textbf{Weak Duality:} $d^* \le p^*$ (Always true). The difference $p^* - d^*$ is the \textit{duality gap}.
    \item \textbf{Strong Duality:} $d^* = p^*$ (Gap is zero). This allows solving the dual problem to find the primal solution.
\end{itemize}

Strong duality is guaranteed if \textbf{Slater's Condition} holds:
\begin{summarybox}{Theorem: Slater's Condition}
    For a \textbf{convex} optimization problem, strong duality holds if there exists a strictly feasible point $\vec{x}$ such that:
    \begin{equation}
        f_i(\vec{x}) < 0 \quad \forall i=1,\dots,m \quad \text{and} \quad A\vec{x} = \vec{b}
    \end{equation}
\end{summarybox}
\textbf{Refinement:} If constraints are affine (linear), touching the boundary ($f_i(\vec{x}) \le 0$) is allowed. Only non-linear constraints require strict inequality.

\subsubsection{Karush-Kuhn-Tucker (KKT) Conditions}
If strong duality holds, any optimal pair $(\vec{x}^*, \vec{\lambda}^*, \vec{\nu}^*)$ \textbf{must} satisfy the KKT conditions:

\begin{enumerate}
    \item \textbf{Primal Feasibility:}
    \[ f_i(\vec{x}^*) \le 0, \quad h_i(\vec{x}^*) = 0 \]

    \item \textbf{Dual Feasibility:} 
    \[ \vec{\lambda}^* \succeq 0 \]

    \item \textbf{Complementary Slackness:} (Crucial for SVMs!)
    \[ \lambda_i^* \cdot f_i(\vec{x}^*) = 0 \]
    \textit{Meaning: Either a constraint is active ($f_i(\vec{x})=0$) or its multiplier is zero ($\lambda_i=0$).}

    \item \textbf{Stationarity:} Gradient of Lagrangian is zero.
    \[ \nabla f_0(\vec{x}^*) + \sum \lambda_i^* \nabla f_i(\vec{x}^*) + \sum \nu_i^* \nabla h_i(\vec{x}^*) = 0 \]
\end{enumerate}

\textbf{Conclusion:} For convex problems with strong duality, KKT conditions are necessary and sufficient for optimality. Finding a point that satisfies them means we found the global optimum.

\newpage