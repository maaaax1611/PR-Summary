\section{Expectation Maximization}
Usually we use Methods like Maximum Likelihood Estimation (MLE) \ref{sec:MLE} or Maximum A-Posteriori (MAP) \ref{sec:MAP} to estimate parameters of probabilistic models.
However, these methods tend to fail when we deal with \textbf{high dimensional data} or \textbf{latent/incomplete variables}.

\subsection{Gaussian Mixture Models}
A Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters.
In mathenatical words: we represent our training data with a set of $K$ Gaussian distributions.
\begin{equation}
    p(\mathbf{x}|\theta) = \sum_{k=1}^{K} p_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \Sigma_k) \quad \text{with} \quad \sum_{k=1}^{K} p_k = 1
\end{equation}
where \( p_k \) are the mixture weights, \( \boldsymbol{\mu}_k \) are the means, and \( \Sigma_k \) are the covariance matrices of the Gaussian components.\\
The individual Gaussian  (probability that the data point $\mathbf{x}_i$ belongs to component $k$) are given by:
\begin{equation}
    p_\text{ik} = \frac{p_\text{k} \mathcal{N}(\mathbf{x}_\text{i}|\boldsymbol{\mu}_k, \Sigma_k)}{p(\mathbf{x}_\text{i})}
\end{equation}
The ML-Estimates for the parameters are given by:
\begin{align}
    \hat{p}_k &= \frac{1}{N} \sum_{i=1}^{N} p_\text{ik} \\
    \hat{\boldsymbol{\mu}}_k &= \frac{\sum_{i=1}^{N} p_\text{ik} \mathbf{x}_\text{i}}{\sum_{i=1}^{N} p_\text{ik}} \\
    \hat{\Sigma}_k &= \frac{\sum_{i=1}^{N} p_\text{ik} (\mathbf{x}_\text{i} - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_\text{i} - \hat{\boldsymbol{\mu}}_k)^T}{\sum_{i=1}^{N} p_\text{ik}}
\end{align}
With all these formulas in hand, we can write out the EM-Algorithm for GMMs:
\begin{algorithm}
\caption{EM Algorithm for GMM parameter estimation}
\begin{algorithmic}[1]
    \State \textbf{Initialization:} $\boldsymbol{\mu}_k^{(0)}, \boldsymbol{\Sigma}_k^{(0)}, p_k^{(0)}$
    \State $j \gets 0$
    \Repeat
        \State \textbf{Expectation step:} compute new values for $p_{ik}, L$
        \State \textbf{Maximization step:} update values for $\boldsymbol{\mu}_k^{(j)}, \boldsymbol{\Sigma}_k^{(j)}, p_k^{(j)}$
        \State $j \gets j + 1$
    \Until{$L$ is no longer changing}
    \State \textbf{Output:} estimates $\hat{\boldsymbol{\mu}}_k, \hat{\boldsymbol{\Sigma}}_k, \hat{p}_k$
\end{algorithmic}
\end{algorithm}

% \subsection{Missing Information Principle}
% EM helps us to find maximum likelihood estimates of parameters in probabilistic models, where the model depends on unobserved latent variables.\\
% The joint probability density of the events (observed data $x$ and latent variables $y$) given by the parameters $\mathbf{\theta} $ is:
% \begin{equation}
%     p(x, y; \mathbf{\theta}) = p(x; \mathbf{\theta}) p(y|x; \mathbf{\theta})
%     \leftrightarrow p(x; \mathbf{\theta}) = \frac{p(x,y;\mathbf{\theta})}{p(y|x;\mathbf{\theta})}
% \end{equation}
% We can take the logarithm and get the mathematical expression for the \textbf{Missing Information Principle}:
% \begin{equation}
%     \label{eq:missing_info_principle}
%     \log p(x; \mathbf{\theta}) = \log p(x,y;\mathbf{\theta}) - \log p(y|x;\mathbf{\theta})
% \end{equation}
% \noindent \textbf{Derivation of an update scheme:}\\
% We now take this formulation and derive an iterative update scheme for the parameters $\mathbf{\theta}$.
% Consider \ref{eq:missing_info_principle} at iteration $i+1$:
% \begin{equation}
%     \log p\left(x; \hat{\mathbf{\theta}}^{(i+1)}\right) = \log p(x,y; \hat{\mathbf{\theta}}^{(i+1)}) - \log p(y|x; \hat{\mathbf{\theta}}^{(i+1)})
% \end{equation}
% We now multiply both sides with $p\left(y|x; \hat{\mathbf{\theta}}^{(i)}\right)$ and integrate over the laten variable/hidden event $y$:
% \begin{align*}
% \int p(y|x; \hat{\theta}^{(i)}) \log p(x; \hat{\theta}^{(i+1)}) \, dy \quad = 
% \quad & \underbrace{\int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \hat{\theta}^{(i+1)}) \, dy}_{Q(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)})} - \\
% & \underbrace{\int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \hat{\theta}^{(i+1)}) \, dy}_{H(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)})}
% \end{align*}
% Now let's first simplify the left side:
% \begin{align*}
% % Line 1: Original integral expression
% \int p(y|x; \hat{\theta}^{(i)}) \log p(x; \hat{\theta}^{(i+1)}) \, dy &= 
% % Line 2: Extracting the log term (constant w.r.t. y) and keeping the trailing equals sign
% \log p(x; \hat{\theta}^{(i+1)}) \underbrace{\int p(y|x; \hat{\theta}^{(i)}) \, dy}_{=1} = \\
% % Line 3: The integral evaluates to 1, leaving the final result
% &= \log p(x; \hat{\theta}^{(i+1)})
% \end{align*}
% This gives us the log likelihood function of the observed data on the left side.

% \noindent Now we analyze the right-hand side of the equation. It consists of two terms:
% \begin{enumerate}
%     \item The expectation of the joint log-likelihood (the so-called $Q$-function).
%     \item A term related to the entropy $H$ of the posterior distribution.
% \end{enumerate}
% So we can rewrite the equation as:
% \begin{equation}
%     \log p(x; \hat{\theta}^{(i+1)}) = Q(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) - H(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)})
% \end{equation}

% \noindent We define the expectation of the complete-data log-likelihood, denoted as $Q(\theta; \hat{\theta}^{(i)})$, as follows:
% \begin{equation}
%     Q(\theta; \hat{\theta}^{(i)}) = \mathbb{E}_{Y|x; \hat{\theta}^{(i)}} [\log p(x, y; \theta)] = \int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \theta) \, dy
% \end{equation}

% \noindent The second term on the right-hand side corresponds to the integral over the conditional log-likelihood. Let's denote this term as $I(\theta)$:
% \begin{equation}
%     I(\theta) = \int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \theta) \, dy
% \end{equation}
% Thus, our log-likelihood equation becomes:
% \begin{equation}
%     \log p(x; \hat{\theta}^{(i+1)}) = Q(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) - I(\hat{\theta}^{(i+1)})
% \end{equation}

% \subsection*{Why is maximizing Q sufficient?}
% To justify that we only need to maximize $Q$, we look at the change in log-likelihood between iterations:
% \[
% \log p(x; \theta) - \log p(x; \hat{\theta}^{(i)}) = \underbrace{\left( Q(\theta; \hat{\theta}^{(i)}) - Q(\hat{\theta}^{(i)}; \hat{\theta}^{(i)}) \right)}_{\Delta Q} - \underbrace{\left( I(\theta) - I(\hat{\theta}^{(i)}) \right)}_{\Delta I}
% \]
% The term $-\Delta I$ can be transformed into the \textbf{Kullback-Leibler Divergence}:
% \begin{align*}
%     - \Delta I &= - \left( \int p(y|x; \hat{\theta}^{(i)}) \log \frac{p(y|x; \theta)}{p(y|x; \hat{\theta}^{(i)})} \, dy \right) \\
%     &= \int p(y|x; \hat{\theta}^{(i)}) \log \frac{p(y|x; \hat{\theta}^{(i)})}{p(y|x; \theta)} \, dy \\
%     &= D_{KL}(p(y|x; \hat{\theta}^{(i)}) || p(y|x; \theta))
% \end{align*}
% Since the KL-divergence is always non-negative ($D_{KL} \geq 0$), the "entropy part" of the equation will never decrease the total likelihood. Therefore, maximizing $Q$ guarantees an increase (or at least no decrease) in the total log-likelihood $\log p(x; \theta)$.

% \noindent \textbf{Conclusion: The EM-Algorithm Update Rule}\\
% Based on this derivation, the algorithm iterates through two steps until convergence:

% \begin{enumerate}
%     \item \textbf{E-Step (Expectation):} Compute the $Q$-function based on the current parameter estimates $\hat{\theta}^{(i)}$:
%     \[
%         Q(\theta; \hat{\theta}^{(i)}) = \int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \theta) \, dy
%     \]
%     \item \textbf{M-Step (Maximization):} Update the parameters by maximizing the $Q$-function:
%     \begin{equation}
%         \hat{\theta}^{(i+1)} = \underset{\theta}{\mathrm{argmax}} \, Q(\theta; \hat{\theta}^{(i)})
%     \end{equation}
% \end{enumerate}
% \newpage

\section{Expectation Maximization}
Usually we use Methods like Maximum Likelihood Estimation (MLE) \ref{sec:MLE} or Maximum A-Posteriori (MAP) \ref{sec:MAP} to estimate parameters of probabilistic models.
However, these methods tend to fail when we deal with \textbf{high dimensional data} or \textbf{latent/incomplete variables}.

\subsection{Gaussian Mixture Models}
A Gaussian Mixture Model (GMM) is a probabilistic model that assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters.
In mathematical words: we represent our training data with a set of $K$ Gaussian distributions.
\begin{equation}
    p(\mathbf{x}|\theta) = \sum_{k=1}^{K} p_k \mathcal{N}(\mathbf{x}|\boldsymbol{\mu}_k, \Sigma_k) \quad \text{with} \quad \sum_{k=1}^{K} p_k = 1
\end{equation}
where \( p_k \) are the mixture weights, \( \boldsymbol{\mu}_k \) are the means, and \( \Sigma_k \) are the covariance matrices of the Gaussian components.\\
The individual Gaussian (probability that the data point $\mathbf{x}_i$ belongs to component $k$) are given by:
\begin{equation}
    p_\text{ik} = \frac{p_\text{k} \mathcal{N}(\mathbf{x}_\text{i}|\boldsymbol{\mu}_k, \Sigma_k)}{p(\mathbf{x}_\text{i})}
\end{equation}
The ML-Estimates for the parameters are given by:
\begin{align}
    \hat{p}_k &= \frac{1}{N} \sum_{i=1}^{N} p_\text{ik} \\
    \hat{\boldsymbol{\mu}}_k &= \frac{\sum_{i=1}^{N} p_\text{ik} \mathbf{x}_\text{i}}{\sum_{i=1}^{N} p_\text{ik}} \\
    \hat{\Sigma}_k &= \frac{\sum_{i=1}^{N} p_\text{ik} (\mathbf{x}_\text{i} - \hat{\boldsymbol{\mu}}_k)(\mathbf{x}_\text{i} - \hat{\boldsymbol{\mu}}_k)^T}{\sum_{i=1}^{N} p_\text{ik}}
\end{align}
With all these formulas in hand, we can write out the EM-Algorithm for GMMs:
\begin{algorithm}
\caption{EM Algorithm for GMM parameter estimation}
\begin{algorithmic}[1]
    \State \textbf{Initialization:} $\boldsymbol{\mu}_k^{(0)}, \boldsymbol{\Sigma}_k^{(0)}, p_k^{(0)}$
    \State $j \gets 0$
    \Repeat
        \State \textbf{Expectation step:} compute new values for $p_{ik}, L$
        \State \textbf{Maximization step:} update values for $\boldsymbol{\mu}_k^{(j)}, \boldsymbol{\Sigma}_k^{(j)}, p_k^{(j)}$
        \State $j \gets j + 1$
    \Until{$L$ is no longer changing}
    \State \textbf{Output:} estimates $\hat{\boldsymbol{\mu}}_k, \hat{\boldsymbol{\Sigma}}_k, \hat{p}_k$
\end{algorithmic}
\end{algorithm}

\subsection{Missing Information Principle}
EM helps us to find maximum likelihood estimates of parameters in probabilistic models, where the model depends on unobserved latent variables.\\
The joint probability density of the events (observed data $x$ and latent variables $y$) given by the parameters $\mathbf{\theta} $ is:
\begin{equation}
    p(x, y; \mathbf{\theta}) = p(x; \mathbf{\theta}) p(y|x; \mathbf{\theta})
    \leftrightarrow p(x; \mathbf{\theta}) = \frac{p(x,y;\mathbf{\theta})}{p(y|x;\mathbf{\theta})}
\end{equation}
We can take the logarithm and get the mathematical expression for the \textbf{Missing Information Principle}:
\begin{equation}
    \label{eq:missing_info_principle}
    \log p(x; \mathbf{\theta}) = \log p(x,y;\mathbf{\theta}) - \log p(y|x;\mathbf{\theta})
\end{equation}

\noindent \textbf{Derivation of an update scheme:}\\
We now take this formulation and derive an iterative update scheme for the parameters $\mathbf{\theta}$.
Consider \ref{eq:missing_info_principle} at iteration $i+1$. We multiply both sides with $p\left(y|x; \hat{\mathbf{\theta}}^{(i)}\right)$ and integrate over the latent variable $y$:

\begin{align*}
\int p(y|x; \hat{\theta}^{(i)}) \log p(x; \hat{\theta}^{(i+1)}) \, dy \quad = 
\quad & \int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \hat{\theta}^{(i+1)}) \, dy - \\
& \int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \hat{\theta}^{(i+1)}) \, dy
\end{align*}

\noindent \textbf{Left Side Simplification:}
Since $\log p(x; \hat{\theta}^{(i+1)})$ does not depend on $y$, we can pull it out of the integral:
\begin{align*}
\int p(y|x; \hat{\theta}^{(i)}) \log p(x; \hat{\theta}^{(i+1)}) \, dy &= 
\log p(x; \hat{\theta}^{(i+1)}) \underbrace{\int p(y|x; \hat{\theta}^{(i)}) \, dy}_{=1} \\
&= \log p(x; \hat{\theta}^{(i+1)})
\end{align*}

\noindent \textbf{Right Side Definitions:}
We define the two terms on the right side as functions of an arbitrary parameter $\theta$:
\begin{enumerate}
    \item The \textbf{Q-function} (Expectation of the complete-data log-likelihood):
    \begin{equation}
        Q(\theta; \hat{\theta}^{(i)}) = \int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \theta) \, dy
    \end{equation}
    
    \item The \textbf{H-term} (Expectation of the posterior log-likelihood):
    \begin{equation}
        H(\theta; \hat{\theta}^{(i)}) = \int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \theta) \, dy
    \end{equation}
\end{enumerate}

\noindent Putting it all together, the decomposition of the log-likelihood for \textit{any} parameter $\theta$ is:
\begin{equation}
    \log p(x; \theta) = Q(\theta; \hat{\theta}^{(i)}) - H(\theta; \hat{\theta}^{(i)})
\end{equation}
Specifically, for the parameters of the next iteration $\hat{\theta}^{(i+1)}$, this holds as:
\begin{equation}
    \log p(x; \hat{\theta}^{(i+1)}) = Q(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) - H(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)})
\end{equation}

\noindent \textbf{Note on Notation:} It is crucial to distinguish the two roles of the parameters in $Q(\theta; \hat{\theta}^{(i)})$:
\begin{itemize}
    \item \textbf{Second argument $\hat{\theta}^{(i)}$ (Fixed):} These are the parameters from the \textit{previous} iteration. They are treated as constant and are used solely to compute the posterior probabilities $p(y|x; \hat{\theta}^{(i)})$ (the "weights" in the expectation).
    \item \textbf{First argument $\theta$ (Variable):} This is the variable we want to optimize. It appears inside the logarithm $\log p(x, y; \theta)$. In the M-Step, we search for the value of $\theta$ that maximizes this function.
\end{itemize}

\subsection*{Why is maximizing Q sufficient?}
To justify that we only need to maximize $Q$, we examine the change in log-likelihood between the new parameter $\hat{\theta}^{(i+1)}$ and the old parameter $\hat{\theta}^{(i)}$:
\[
\log p(x; \hat{\theta}^{(i+1)}) - \log p(x; \hat{\theta}^{(i)}) = \underbrace{\left( Q(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) - Q(\hat{\theta}^{(i)}; \hat{\theta}^{(i)}) \right)}_{\Delta Q} - \underbrace{\left( H(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) - H(\hat{\theta}^{(i)}; \hat{\theta}^{(i)}) \right)}_{\Delta H}
\]
The term $-\Delta H$ corresponds to the difference of the H-terms. We can rewrite this difference:
\begin{align*}
    -\Delta H &= H(\hat{\theta}^{(i)}; \hat{\theta}^{(i)}) - H(\hat{\theta}^{(i+1)}; \hat{\theta}^{(i)}) \\
    &= \int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \hat{\theta}^{(i)}) \, dy - \int p(y|x; \hat{\theta}^{(i)}) \log p(y|x; \hat{\theta}^{(i+1)}) \, dy \\
    &= \int p(y|x; \hat{\theta}^{(i)}) \log \frac{p(y|x; \hat{\theta}^{(i)})}{p(y|x; \hat{\theta}^{(i+1)})} \, dy
\end{align*}
This expression is exactly the \textbf{Kullback-Leibler Divergence}:
\[
    -\Delta H = D_{KL}(p(y|x; \hat{\theta}^{(i)}) || p(y|x; \hat{\theta}^{(i+1)}))
\]
Since the KL-divergence is always non-negative ($D_{KL} \geq 0$), the term $-\Delta H$ is always $\geq 0$. 
This means the "entropy part" never decreases the total likelihood gain. Therefore, maximizing $Q$ (making $\Delta Q$ positive) guarantees that the total log-likelihood increases.

\noindent \textbf{Conclusion: The EM-Algorithm Update Rule}\\
\begin{algorithm}
\caption{Expectation Maximization (EM) Algorithm}
\begin{algorithmic}[1] % The [1] enables line numbering
    \State \textbf{Initialization:} Choose starting parameters $\hat{\theta}^{(0)}$
    \State $i \gets -1$
    \Repeat
        \State $i \gets i+1$
        
        \State \textbf{Expectation step:}
        \State \quad Calculate the $Q$-function using current parameters $\hat{\theta}^{(i)}$:
        \[
            Q(\theta; \hat{\theta}^{(i)}) := \int p(y|x; \hat{\theta}^{(i)}) \log p(x, y; \theta) \, dy
        \]
        
        \State \textbf{Maximization step:}
        \State \quad Find the new parameters $\hat{\theta}^{(i+1)}$ that maximize $Q$:
        \[
            \hat{\theta}^{(i+1)} \gets \underset{\theta}{\mathrm{argmax}} \, Q(\theta; \hat{\theta}^{(i)})
        \]
        
    \Until{$\hat{\theta}^{(i+1)} \approx \hat{\theta}^{(i)}$} \Comment{Check for convergence}
    
    \State \textbf{Output:} estimate $\hat{\theta} \gets \hat{\theta}^{(i)}$
\end{algorithmic}
\end{algorithm}

\noindent \textbf{{\color{green}Pros} and {\color{red}Cons} of EM:}
\begin{itemize}
    \item {\color{green}Pros:}
    \begin{itemize}
        \item Often closed form iteration schemes can be found
        \item Numerically robust iteration scheme
        \item Constant memory requirements if closed form solutions exist
    \end{itemize}
    \item {\color{red}Cons:}
    \begin{itemize}
        \item Can converge to local maxima, depending on initialization.
        \item May require many iterations to converge, especially for complex models (slow slow slow).
    \end{itemize}
\end{itemize}

\noindent Many optimization problems with EM follow the constraint, that $\sum_{k=1}^K p_k = 1$ and $p_k \geq 0$.
This can be solved with the method of Lagrange multipliers \ref{sec:lagrangian}.
\newpage