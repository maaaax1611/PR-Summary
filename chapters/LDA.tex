\section{Discriminant Analysis}
\subsection{Motivation}
From chapter \ref{sec:bayesian_classifier} we know, that the decison boundary of Gaussian classifiers is quadratic in general. 
However, if we assume that all classes share the same covariance matrix and $\Sigma = I$ (Identity matrix), the decision boundary not only becomes linear, but we get a Nearest Neighbour classifier.
Based on this we seek to find a transformation that maps the data into a space where the class covariances are equal and the Identity matrix.\\
Another observation we made is, that lifting feature vectors into higher dimensions can result in a linear decision boundary.\\
And if we already are in higher dimensions, why not try to find a lower-dimensional space that still allows for good class separation?\\
\textbf{Goal:}
\begin{enumerate}
    \item Transformation that gives $\Sigma = I$ for all classes
    \item Lower dimensional space with good class separability
    \item Linear decision boundary in the transformed space
\end{enumerate}
\subsection{From quadratic to linear decision boundaries}
By using SVD we can decompose the covariance matrix of the data as follows:
\begin{equation}
    \mathbf{\Sigma} = \mathbf{U} \mathbf{D} \mathbf{U}^T = (\mathbf{U} \mathbf{D}^{1/2})(\mathbf{U} \mathbf{D}^{1/2})^T = (\mathbf{U} \mathbf{D}^{1/2})\cdot \mathbf{I} \cdot(\mathbf{U} \mathbf{D}^{1/2})^T
\end{equation}
Where $\mathbf{U}$ is the orthogonal matrix of eigenvectors and $\mathbf{D}$ is the diagonal matrix of eigenvalues.\\
We want to incorporate this into the Gaussian density function. This requires to compute the inverse and determinant of $\mathbf{\Sigma}$:
\begin{equation}
    \mathbf{\Sigma}^{-1} = \mathbf{U} \mathbf{D}^{-1} \mathbf{U}^T = (\mathbf{U} \mathbf{D}^{-1/2})(\mathbf{U} \mathbf{D}^{-1/2})^T
\end{equation}
\begin{equation}
    \text{det} \mathbf{\Sigma} = \prod_{i}^{d} d_{i,i}
\end{equation}
Now we can rewrite the Gaussian density function as follows:
\begin{align*}
    \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}, \Sigma) 
    &= \frac{1}{\sqrt{\det 2\pi\Sigma}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x}-\boldsymbol{\mu})} \\
    &= \frac{1}{\sqrt{\det 2\pi\Sigma}} e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^T \underbrace{(UD^{-\frac{1}{2}}) \cdot I \cdot (UD^{-\frac{1}{2}})^T}_{\Sigma^{-1}} (\mathbf{x}-\boldsymbol{\mu})} \\
    &= \frac{1}{\sqrt{\det 2\pi\Sigma}} e^{-\frac{1}{2} \left( \underbrace{(D^{-\frac{1}{2}}U^T)\mathbf{x}}_{\text{transformed } \vec{x}} - \underbrace{(D^{-\frac{1}{2}}U^T)\boldsymbol{\mu}}_{\text{transformed } \vec{\mu}} \right)^T \underbrace{I}_{\substack{\text{Covariance matrix} \\ \text{becomes an Identity matrix} \\ \Sigma = 1}} \left( \underbrace{(D^{-\frac{1}{2}}U^T)\mathbf{x}}_{\text{transformed } \vec{x}} - \underbrace{(D^{-\frac{1}{2}}U^T)\boldsymbol{\mu}}_{\text{transformed } \vec{\mu}} \right)}
\end{align*}
From this we can read off the transformation that maps the data into a space where the covariance matrix is the Identity matrix:
\begin{equation}
    x' = \Phi (x) = \mathbf{D}^{-\frac{1}{2}}_y \mathbf{U}^T_y \mathbf{x}
\end{equation}
The transformation $\Phi(x)$ is called \textbf{whitening transformation}.\\
We can also show that $\mathbf{x'}$ is normally distributed:
\begin{equation}
    p(\mathbf{x'}|y) = \mathcal{N}(\mathbf{x'}; \boldsymbol{\mu_y'}, \Sigma_y') = \mathcal{N}(\mathbf{x'}; \mathbf{D}^{-\frac{1}{2}}_y \mathbf{U}^T_y \boldsymbol{\mu_y}, \mathbf{I})
\end{equation}
\subsection{Linear Discriminant Analysis (LDA)}
What we have seen above has one big disadvantage: The transformation is class dependent! This means each class gets mapped into its own space. To solve this problem we compute the transformation based 
on the joint covariance matrix:
\begin{enumerate}
    \item \textbf{ML estimation of the joint covariance matrix:}
    \begin{equation*}
        \widehat{\Sigma} = \frac{1}{m} \sum_{i=1}^{m} (\mathbf{x}_i - \boldsymbol{\mu}_{y_i})(\mathbf{x}_i - \boldsymbol{\mu}_{y_i})^T
    \end{equation*}
    \item \textbf{Compute SVD of covariance matrix:} 
    \begin{equation*}
        \widehat{\Sigma} = U D U^T
    \end{equation*}
    \item \textbf{Assign transform:}
    \begin{equation*}
        \phi = D^{-\frac{1}{2}} U^T
    \end{equation*}
    \item \textbf{Compute mean vectors for all } $y$
    \begin{equation*}
        \boldsymbol{\mu}'_y = \phi(\boldsymbol{\mu}_y) = D^{-\frac{1}{2}} U^T \boldsymbol{\mu}_y
    \end{equation*}
\end{enumerate}
\noindent \textbf{Output:} feature transform $\phi$, transformed mean vectors $\boldsymbol{\mu}'_y$\\

\textbf{Decision Rule:}
\begin{align*}
    y^* &= \operatorname*{argmax}_y p(y \mid \phi(\mathbf{x})) \\
        &= \operatorname*{argmax}_y \left\{ \log p(y) - \frac{1}{2} (\phi(\mathbf{x}) - \phi(\boldsymbol{\mu}_y))^T (\phi(\mathbf{x}) - \phi(\boldsymbol{\mu}_y)) \right\} \\
        &= \operatorname*{argmin}_y \left\{ \frac{1}{2} \| \phi(\mathbf{x}) - \phi(\boldsymbol{\mu}_y) \|_2^2 - \log p(y) \right\}
\end{align*}
\noindent where $\| \cdot \|_2$ denotes the $L_2$ norm. Note that this basically is a biased nearest neighbour classifier in the transformed space.\\
This gets clearer when looking at the following visualization:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{LDA_NN_2class.png}
    \caption{Nearest Neighbour classification for two classes}
    \label{fig:lda_NN_2class}
\end{figure}
The classification of a new LDA-transformed sample $\Phi(\mathbf{x})$ only depends on the distance of the projected sample on a line connecting the two class means $\boldsymbol{\mu_0'}$ and $\boldsymbol{\mu_1'}$. The direction orthogonal to this line is not relevant for classification.\\
Based on this observation we can also use the angle between the transformed sample $\Phi(\mathbf{x})$ and $\boldsymbol{\mu_1'} - \boldsymbol{\mu_0'}$ for classification.
Our decision rule can also be written as:
\begin{equation*}
    y^* = 
    \begin{cases} 
        0, & \text{if } \phi(\mathbf{x})^T (\phi(\boldsymbol{\mu}_1) - \phi(\boldsymbol{\mu}_0)) < \frac{1}{2} (\phi(\boldsymbol{\mu}_1)^T \phi(\boldsymbol{\mu}_1) - \phi(\boldsymbol{\mu}_0)^T \phi(\boldsymbol{\mu}_0)) \\
        1, & \text{otherwise.}
    \end{cases}
\end{equation*}
At a first look this might seem overwhelming, but all we do is projecting $\Phi(\mathbf{x})$ onto the line connecting the two class means and makeing a decision based on which side of the midpoint the projection lies.

\subsection{Rank-Reduced Linear Discriminant Analysis}
The main target of Rank-Reduced LDA is to find a transformation $\Phi$ that projects features into a lower-dimensional space ($L < K-1$) where the spread (variance) of the features is maximized.
Basically we try to maximize the distance between class means. In terms of an optimization problem this can be written as follows:
\begin{equation*}
    \mathbf{\Phi}^* = \operatorname*{argmax}_{\mathbf{\Phi}} \left( \frac{1}{K} \sum_{y=1}^{K} (\mathbf{\Phi} \boldsymbol{\mu}'_y - \mathbf{\Phi} \bar{\boldsymbol{\mu}}')^T (\mathbf{\Phi} \boldsymbol{\mu}'_y - \mathbf{\Phi} \bar{\boldsymbol{\mu}}') + \sum_{i=1}^{L} \lambda_i (\|\mathbf{\Phi}_i\|_2^2 - 1) \right)
\end{equation*}
\noindent where $\bar{\boldsymbol{\mu}}' = \frac{1}{K} \sum_{y=1}^{K} \boldsymbol{\mu}'_y$ is the global mean of all class means and $\mathbf{\Phi}_i$ is the $i$-th row of the transformation matrix $\mathbf{\Phi}$. The constraints $\|\mathbf{\Phi}_i\|_2^2 = 1$ ensure that the rows of $\mathbf{\Phi}$ are unit vectors and are necessary because $\Phi \to \infty$ would solve.\\
The solution to this optimization problem is given by an eigenvector-eigenvalue problem.
\begin{equation}
    \mathbf{\Sigma_\text{inter}} \mathbf{\Phi}^T = \mathbf{\lambda'} \mathbf{\Phi}^T
\end{equation}
\noindent where $\mathbf{\Sigma_\text{inter}}$ is the inter-class scatter matrix, $\mathbf{\Phi}$ is the transformation matrix and $\mathbf{\lambda'}$ is the diagonal matrix of eigenvalues.\\

\noindent \textbf{Input:} training data: $S = \{(\mathbf{x}_1, y_1), (\mathbf{x}_2, y_2), (\mathbf{x}_3, y_3), \dots, (\mathbf{x}_m, y_m)\}$

\begin{enumerate}
    \item Compute the \textbf{covariance matrix of transformed mean vectors}
    \begin{equation*}
        \widehat{\Sigma}_{\text{inter}} = \frac{1}{K} \sum_{y=1}^{K} (\boldsymbol{\mu}'_y - \bar{\boldsymbol{\mu}}')(\boldsymbol{\mu}'_y - \bar{\boldsymbol{\mu}}')^T,
    \end{equation*}
    
    where $\bar{\boldsymbol{\mu}}' = \frac{1}{K} \cdot \sum_{y=1}^{K} \boldsymbol{\mu}'_y$.

    \item Compute the $L$ \textbf{eigenvectors of the covariance matrix} belonging to the largest eigenvalues.

    \item The \textbf{eigenvectors are the rows} of the mapping $\mathbf{\Phi}$ from the $(K-1)$- to the $L$-dimensional feature space.
\end{enumerate}

\noindent \textbf{Output:} matrix $\mathbf{\Phi}$


\subsection{Fisher Transform}
From the postulates or PR we know that it is desireable to have small intra-class distances and large inter-class distances.
The Fisher transform tries to find a transformation that optimizes this criterion.
We define the following optimization problem:
\begin{equation*}
    \boldsymbol{r}^* = \operatorname*{argmax}_{\boldsymbol{r}} J(\boldsymbol{r}) = \operatorname*{argmax}_{\boldsymbol{r}} \frac{|\tilde{\mu}_1 - \tilde{\mu}_2|^2}{\tilde{s}_1^2 + \tilde{s}_2^2}
\end{equation*}
With a bit of maths this can be reformulated to:
\begin{equation*}
    \boldsymbol{a}^* = \operatorname*{argmax}_{\boldsymbol{a}} \frac{\boldsymbol{a}^T \Sigma_{\text{inter}} \boldsymbol{a}}{\boldsymbol{a}^T \Sigma_{\text{intra}} \boldsymbol{a}}
\end{equation*}
Formally this describes the maximization of the Rayleigh ratio for each projection axis $\boldsymbol{a}^*$.

\subsection{Comments on Dimensionality Reduction}
\begin{itemize}
    \item PCA does not rquire class labels, LDA does
    \item PCA transformed features are approximately normally distributed (central limit theorem)
    \item Since LDA is a linear transformation, the transformed features are normally distributed if the original features are normally distributed
    \item Components of PCA transformed features are uncorrelated and mutually independent
    \item \textbf{John-Lindenstrauss-lemma: }If vectors are projected onto a randomly chosen subspace of sufficient dimension, then the distances between the points are approximately preserved.
\end{itemize}
\newpage