\section{Kernels \& Kernel Methods}

\subsection{Feature Transforms}
\label{feature_transforms}
Linear decision boundaries have limitations (cannot classify non-linearly separable data).\\
\textbf{Solution:} Map data into a higher-dimensional feature space using a non-linear transformation $\phi: \mathbb{R}^d \to \mathbb{R}^D$ ($D \ge d$).

\textbf{Example:} Mapping 2D data to 2D (squaring):
\begin{equation}
    \phi(\vec{x}) = (x_1^2, x_2^2)^T
\end{equation}

\begin{figure}[!ht]
    \centering
    % Achte darauf, dass der Pfad stimmt (images/...)
    \includegraphics[width=\textwidth]{FeatureTransforms_square.PNG}
    \caption{Feature transform: squaring each coordinate}
    \label{fig:feature_transform_square}
\end{figure}


Data that forms a circle in the original space becomes linearly separable in the transformed space.

\textbf{Property:} Distances in the transformed space can be computed using only inner products:
\begin{align}
    \|\phi(\vec{x}) - \phi(\vec{x}')\|_2^2 &= \langle (\phi(\vec{x}) - \phi(\vec{x}')), (\phi(\vec{x}) - \phi(\vec{x}')) \rangle \\
    &= \langle \phi(\vec{x}), \phi(\vec{x}) \rangle - 2\langle \phi(\vec{x}), \phi(\vec{x}') \rangle + \langle \phi(\vec{x}'), \phi(\vec{x}') \rangle
\end{align}
This observation leads us to Kernel Functions.

\subsection{Kernel Functions \& The Kernel Trick}
\begin{summarybox}{Definition: Kernel Function}
    A kernel function $k: \mathcal{X} \times \mathcal{X} \to \mathbb{R}$ is a symmetric function that maps a pair of features to a real number. It corresponds to an inner product in some feature space:
    \begin{equation}
        k(\vec{x}, \vec{x}') = \langle \phi(\vec{x}), \phi(\vec{x}') \rangle
    \end{equation}
\end{summarybox}

\textbf{The Kernel Trick:} In any algorithm formulated in terms of inner products (like SVM or PCA), we can replace the inner product with a kernel $k(\vec{x}, \vec{x}')$. This avoids the explicit (and expensive) computation of $\phi(\vec{x})$.

\textbf{Mercer's Theorem:} For any positve semidefinite kernel function $k$, there exists a feature mapping $\phi$ such that $k(\vec{x}, \vec{x}') = \langle \phi(\vec{x}), \phi(\vec{x}') \rangle$ (kernel function is fully defined by inner products).

\textbf{Typical Kernel Functions}
\begin{itemize}
    \item \textbf{Linear:} $k(\vec{x}, \vec{x}') = \langle \vec{x}, \vec{x}' \rangle$
    \item \textbf{Polynomial:} $k(\vec{x}, \vec{x}') = (\langle \vec{x}, \vec{x}' \rangle + 1)^d$
    \item \textbf{RBF (Radial Basis Function):} $k(\vec{x}, \vec{x}') = \exp\left(-\frac{\|\vec{x} - \vec{x}'\|^2}{\sigma^2}\right)$
    \item \textbf{Sigmoid:} $k(\vec{x}, \vec{x}') = \tanh(\alpha \langle \vec{x}, \vec{x}' \rangle + \beta)$
\end{itemize}

\textbf{Kernel Matrix}
\begin{summarybox}{Definition: Kernel Matrix}
    For a given set of feature vectors $\vec{x}_1, \vec{x}_2, \dots, \vec{x}_m$, we define the \textbf{Kernel Matrix} $K \in \mathbb{R}^{m \times m}$ as:
    \begin{equation}
        K_{i,j} = k(\vec{x}_i, \vec{x}_j) = \langle \phi(\vec{x}_i), \phi(\vec{x}_j) \rangle
    \end{equation}
\end{summarybox}

\textbf{Important Properties:}
\begin{itemize}
    \item The entries of the matrix can be interpreted as \textbf{similarity measures} between the feature pairs (because the inner product (german: Skalarprodukt) measures similarity).
    \item The kernel matrix is always \textbf{positive semidefinite} ($K \succeq 0$). This is a necessary condition for a function to be a valid kernel (Mercer's Theorem).
\end{itemize}

\subsection{Kernel SVM}
In chapter \ref{SVM}, we saw, that standard SVMs can only learn linear decision boundaries.
By applying the kernel trick, we can extend SVMs to learn non-linear decision boundaries.
Since the decision rule \ref{SVM_decision_rule_final} and the dual optimization \ref{svm_dual_final} problem depend only on inner products, we can replace them with kernel functions.
The decision rule becomes:
\begin{equation}
    f(\vec{u}) = \sum_{i=1}^m \lambda_i y_i k(\vec{x}_i, \vec{u}) + \alpha_0
\end{equation}
The dual optimization problem becomes:
\begin{equation}
    \max_{\vec{\lambda}} \sum_{i=1}^m \lambda_i - \frac{1}{2} \sum_{i=1}^m \sum_{j=1}^m \lambda_i \lambda_j y_i y_j k(\vec{x}_i, \vec{x}_j)
\end{equation}
The choice of the kernel function $k$ determines the type of non-linear decision boundary that can be learned.
\begin{figure}[h!]
    \centering
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{Kernel_SVM_poly.PNG}
        \caption{Polynomial Kernel Decision Boundary}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{Kernel_SVM_gauss.PNG}
        \caption{RBF (Gaussian) Kernel Decision Boundary}
    \end{minipage}
\end{figure}
\textbf{Result:} By choosing different kernel functions, we can learn a variety of non-linear decision boundaries without explicitly computing the feature transformation $\phi$.

\subsection{Kernel PCA}
Standard PCA finds principal components by diagonalizing the covariance matrix $\Sigma$. In Kernel PCA, we perform PCA in the feature space defined by $\phi$.
This allows us to capture non-linear structures in the data.
Also the computatuional 

\subsubsection{PCA Revisited}
The covariance matrix in feature space is (assuming zero mean):
\begin{equation}
    \Sigma = \frac{1}{m} \sum_{j=1}^m \phi(\vec{x}_j) \phi(\vec{x}_j)^T
\end{equation}
We solve the eigenvalue problem: $\Sigma \vec{e}_i = \lambda_i \vec{e}_i$.

\subsubsection{Rewriting the Eigenvalue Problem}
The eigenvectors $\vec{e}_i$ lie in the span of the feature vectors and can therefore be expressed as a linear combination of feature vectors:
\begin{equation}
    \vec{e}_i = \sum_{k=1}^m \alpha_{i,k} \vec{x}_k
\end{equation}
Substituting this into the eigenvalue equation:
\begin{equation}
    \left( \frac{1}{m} \sum_{j=1}^m \vec{x}_j \vec{x}_j^T \right) \cdot \sum_{k} \alpha_{i,k} \vec{x}_k = \lambda_i \sum_{k} \alpha_{i,k} \vec{x}_k
\end{equation}
To introduce the kernel, we multiply from the left by $\vec{x}_l^T$ for all $l$:
\begin{equation}
    \sum_{j,k} \alpha_{i,k} \underbrace{\vec{x}_l^T \vec{x}_j}_{k(\vec{x}_l, \vec{x}_j)} \underbrace{\vec{x}_j^T \vec{x}_k}_{k(\vec{x}_j, \vec{x}_k)} = m \lambda_i \sum_{k} \alpha_{i,k} \underbrace{\vec{x}_l^T \vec{x}_k}_{k(\vec{x}_l, \vec{x}_k)}
\end{equation}

Since all feature vectors only appear in the form of inner products, we can replace them with the kernel function $k$:
\begin{summarybox}{Kernel PCA Equation}
    \begin{equation}
        \sum_{j,k} \alpha_{i,k} k(\vec{x}_l, \vec{x}_j) k(\vec{x}_j, \vec{x}_k) = m \lambda_i \sum_{k} \alpha_{i,k} k(\vec{x}_l, \vec{x}_k)
    \end{equation}
    In matrix notation using the Kernel Matrix $K$:
    \begin{equation}
        K^2 \vec{\alpha}_i = m \lambda_i K \vec{\alpha}_i
    \end{equation}
    This implies:
    \begin{equation}
        K \vec{\alpha}_i = m \lambda_i \vec{\alpha}_i
    \end{equation}
    So now we only need to solve the eigenvalue problem for the kernel matrix $K$!
\end{summarybox}

\subsubsection{Projection and Centering}
\begin{itemize}
    \item \textbf{Computation:} We solve the eigenvalue problem for the ($m \times m$) kernel matrix $K$.
    \item \textbf{Projection:} The projection $c$ of a transformed feature vector $\phi(\vec{x})$ onto the principal component $\vec{e}_i$ is easily computed by:
    \begin{equation}
        c = \phi(\vec{x})^T \vec{e}_i = \sum_{k} \alpha_{i,k} k(\vec{x}, \vec{x}_k)
    \end{equation}
    \item \textbf{Centering:} It is assumed that transformed features have zero mean. This is enforced by centering the kernel matrix:
    \[ \tilde{K}_{i,j} = \tilde{\phi}(\vec{x}_i)^T \tilde{\phi}(\vec{x}_j) = K_{i,j} - \frac{1}{m}\sum_k K_{i,k} - \frac{1}{m}\sum_k K_{k,j} - \frac{1}{m}\sum_k K_{k,l} \]
\end{itemize}

\subsection{Kernels for Sequences}
Kernels allow us to apply SVMs/PCA to non-vectorial data like text or speech.
\begin{itemize}
    \item \textbf{String Kernels / DTW:} Based on Dynamic Time Warping distance $D$.
    \begin{equation}
        k(\vec{x}, \vec{y}) = \exp(-D(\vec{x}, \vec{y}))
    \end{equation}
    \item \textbf{Fisher Kernels:} Combine generative models (e.g., HMMs) with discriminative classifiers.
    \begin{equation}
        k(\vec{x}, \vec{x}') = \mathbf{J}_{\theta}(\vec{x})^T \mathbf{I}^{-1} \mathbf{J}_{\theta}(\vec{x}')
    \end{equation}
    where $\mathbf{J}_{\theta}$ is the Fisher Score (gradient of log-likelihood) and $\mathbf{I}$ is the Fisher Information Matrix[cite: 5626].
\end{itemize}